# analyse_stats_light.py - Version LSD sans graphiques
import pandas as pd
import numpy as np
from scipy import stats
import glob
import sys
import os
import json
import argparse


def main():
    # Parser les arguments de ligne de commande
    parser = argparse.ArgumentParser(
        description="Analyse des statistiques de matchs d'arch√©types"
    )
    parser.add_argument(
        "stats_dir",
        nargs="?",
        default="./stats_local",
        help="Chemin vers le r√©pertoire contenant les fichiers stats (d√©faut: ./stats_local)",
    )
    parser.add_argument(
        "--output-sample",
        default="sample_for_viz.csv",
        help="Fichier de sortie pour l'√©chantillon (d√©faut: sample_for_viz.csv)",
    )
    parser.add_argument(
        "--output-json",
        default="stats_results.json",
        help="Fichier de sortie pour les statistiques JSON (d√©faut: stats_results.json)",
    )
    parser.add_argument(
        "--sample-size",
        type=int,
        default=100000,
        help="Taille maximale de l'√©chantillon (d√©faut: 100000)",
    )

    args = parser.parse_args()

    stats_dir = args.stats_dir
    output_sample = args.output_sample
    output_json = args.output_json
    max_sample_size = args.sample_size

    print(f"üìÇ Lecture des fichiers depuis {stats_dir}...")

    # V√©rifier que le r√©pertoire existe
    if not os.path.exists(stats_dir):
        print(f"‚ùå Le r√©pertoire {stats_dir} n'existe pas!")
        print("\nUtilisation:")
        print(
            f"  python3 {sys.argv[0]} <stats_dir> [--output-sample FILE] [--output-json FILE] [--sample-size N]"
        )
        print("\nExemple:")
        print(f"  python3 {sys.argv[0]} ./stats_local")
        print(f"  python3 {sys.argv[0]} /path/to/stats --sample-size 50000")
        sys.exit(1)

    # Lire tous les fichiers
    files = glob.glob(os.path.join(stats_dir, "part-*"))
    if not files:
        print(f"‚ùå Aucun fichier part-* trouv√© dans {stats_dir}")
        sys.exit(1)

    print(f"üìÇ Lecture de {len(files)} fichier(s)...")

    # Lecture par chunks pour √©conomiser la m√©moire
    chunks = []
    for f in files:
        try:
            # Lire par petits morceaux
            for chunk in pd.read_csv(
                f,
                sep=";",
                names=[
                    "source",
                    "target",
                    "count",
                    "wins",
                    "count_source",
                    "count_target",
                    "prediction",
                ],
                chunksize=100000,  # Lire 100k lignes √† la fois
            ):
                chunks.append(chunk)
        except Exception as e:
            print(f"‚ö†Ô∏è  Erreur lors de la lecture de {f}: {e}")

    if not chunks:
        print("‚ùå Aucune donn√©e charg√©e!")
        sys.exit(1)

    print("üîÑ Concatenation des donn√©es...")
    df = pd.concat(chunks, ignore_index=True)

    print(f"\n{'='*60}")
    print(f"üìä STATISTIQUES DESCRIPTIVES")
    print(f"{'='*60}")
    print(f"Total edges: {len(df):,}")
    print(f"Arch√©types uniques (source): {df['source'].nunique():,}")
    print(f"Arch√©types uniques (target): {df['target'].nunique():,}")
    print(f"\n{df[['count', 'prediction']].describe()}")

    # R√©gression lin√©aire
    print(f"\n{'='*60}")
    print(f"üìà R√âGRESSION LIN√âAIRE")
    print(f"{'='*60}")

    slope, intercept, r_value, p_value, std_err = stats.linregress(
        df["prediction"], df["count"]
    )

    print(f"√âquation: y = {slope:.4f}x + {intercept:.4f}")
    print(f"Coefficient de corr√©lation (r): {r_value:.4f}")
    print(f"Coefficient de d√©termination (R¬≤): {r_value**2:.4f}")
    print(f"P-value: {p_value:.4e}")
    print(f"Erreur standard: {std_err:.4f}")

    # R√©sidus
    print(f"\n{'='*60}")
    print(f"üìä ANALYSE DES R√âSIDUS")
    print(f"{'='*60}")

    residuals = df["count"] - df["prediction"]
    print(f"Moyenne des r√©sidus: {residuals.mean():.4f}")
    print(f"√âcart-type des r√©sidus: {residuals.std():.4f}")
    print(f"M√©diane des r√©sidus: {residuals.median():.4f}")
    print(f"Min r√©sidu: {residuals.min():.4f}")
    print(f"Max r√©sidu: {residuals.max():.4f}")

    # Top arch√©types
    print(f"\n{'='*60}")
    print(f"üèÜ TOP 20 ARCH√âTYPES LES PLUS JOU√âS")
    print(f"{'='*60}")

    top_archetypes = (
        df.groupby("source")["count_source"]
        .first()
        .sort_values(ascending=False)
        .head(20)
    )

    for i, (arch, count) in enumerate(top_archetypes.items(), 1):
        print(f"  {i:2d}. {arch}: {count:,} matchs")

    # Matrice de corr√©lation
    print(f"\n{'='*60}")
    print(f"üîó MATRICE DE CORR√âLATION")
    print(f"{'='*60}")

    corr = df[
        ["count", "wins", "count_source", "count_target", "prediction"]
    ].corr()
    print(corr.to_string())

    # Sauvegarder les donn√©es pour visualisation ult√©rieure
    print(f"\n{'='*60}")
    print(f"üíæ SAUVEGARDE DES DONN√âES POUR VISUALISATION")
    print(f"{'='*60}")

    # √âchantillonner les donn√©es pour la visualisation
    sample_size = min(max_sample_size, len(df))
    df_sample = df.sample(n=sample_size, random_state=42)

    # Sauvegarder en CSV l√©ger
    df_sample.to_csv(output_sample, index=False)
    print(
        f"‚úì {sample_size:,} points √©chantillonn√©s sauvegard√©s dans {output_sample}"
    )

    # Sauvegarder les statistiques en JSON
    results = {
        "total_edges": int(len(df)),
        "unique_source": int(df["source"].nunique()),
        "unique_target": int(df["target"].nunique()),
        "regression": {
            "slope": float(slope),
            "intercept": float(intercept),
            "r_value": float(r_value),
            "r_squared": float(r_value**2),
            "p_value": float(p_value),
            "std_err": float(std_err),
        },
        "residuals": {
            "mean": float(residuals.mean()),
            "std": float(residuals.std()),
            "median": float(residuals.median()),
            "min": float(residuals.min()),
            "max": float(residuals.max()),
        },
        "top_archetypes": {k: int(v) for k, v in top_archetypes.items()},
        "correlation_matrix": corr.to_dict(),
    }

    with open(output_json, "w") as f:
        json.dump(results, f, indent=2)
    print(f"‚úì Statistiques sauvegard√©es dans {output_json}")

    print(f"\n{'='*60}")
    print("‚úÖ Analyse termin√©e avec succ√®s!")
    print(f"{'='*60}")


if __name__ == "__main__":
    main()
