# analyse_stats_light_optimized.py
import pandas as pd
import numpy as np
from scipy import stats
import glob
import sys
import os
import json
import argparse
import random

def main():
    parser = argparse.ArgumentParser(description="Analyse des statistiques de matchs d'archÃ©types")
    parser.add_argument("stats_dir", nargs="?", default="./stats_local", help="Chemin rÃ©pertoire stats")
    parser.add_argument("--output-sample", default="sample_for_viz.csv", help="Fichier sortie Ã©chantillon")
    parser.add_argument("--output-json", default="stats_results.json", help="Fichier sortie JSON")
    
    # J'ai ajoutÃ© un argument pour contrÃ´ler le taux de lecture
    parser.add_argument("--read-fraction", type=float, default=0.1, 
                        help="Fraction des donnÃ©es Ã  lire (ex: 0.1 pour 10%). 1.0 pour tout lire.")
    
    parser.add_argument("--sample-size", type=int, default=100000, help="Taille de l'Ã©chantillon final sauvegardÃ©")

    args = parser.parse_args()

    stats_dir = args.stats_dir
    output_sample = args.output_sample
    output_json = args.output_json
    read_fraction = args.read_fraction # Nouveau paramÃ¨tre
    max_sample_size = args.sample_size

    print(f"ğŸ“‚ Lecture des fichiers depuis {stats_dir}...")
    
    if not os.path.exists(stats_dir):
        print(f"âŒ RÃ©pertoire introuvable: {stats_dir}")
        sys.exit(1)

    files = glob.glob(os.path.join(stats_dir, "part-*"))
    if not files:
        print(f"âŒ Aucun fichier part-* trouvÃ©")
        sys.exit(1)

    print(f"ğŸ“‚ Lecture de {len(files)} fichier(s)...")
    if read_fraction < 1.0:
        print(f"âš ï¸  Mode Ã‰CONOMIE DE RAM : Lecture de {read_fraction*100}% des donnÃ©es alÃ©atoirement.")

    # Optimisation des types pour rÃ©duire la RAM par 2
    dtypes = {
        "source": "string", # Ou 'category' si peu de valeurs uniques, mais 'string' est plus sÃ»r ici
        "target": "string",
        "count": "int32",       # int32 suffit pour des milliards (jusqu'Ã  2e9)
        "wins": "int32",
        "count_source": "int32",
        "count_target": "int32",
        "prediction": "float32" # float32 suffit largement pour des probas
    }

    chunks = []
    total_rows_seen = 0
    
    for f in files:
        try:
            # On lit par morceaux
            for chunk in pd.read_csv(
                f,
                sep=";",
                names=["source", "target", "count", "wins", "count_source", "count_target", "prediction"],
                dtype=dtypes, # Application des types optimisÃ©s
                chunksize=100000,
                on_bad_lines='skip' # Ã‰vite de planter sur une ligne corrompue
            ):
                # Ã‰CHANTILLONNAGE Ã€ LA VOLÃ‰E
                # Si on a trop de donnÃ©es, on ne garde qu'une fraction alÃ©atoire DU CHUNK
                if read_fraction < 1.0:
                    chunk = chunk.sample(frac=read_fraction, random_state=42)
                
                chunks.append(chunk)
                total_rows_seen += len(chunk)
                
                # SÃ©curitÃ© : Si on dÃ©passe 50 millions de lignes en mÃ©moire, on prÃ©vient
                if total_rows_seen > 50_000_000 and len(chunks) % 100 == 0:
                    print(f"   ... {total_rows_seen:,} lignes chargÃ©es en mÃ©moire ...")

        except Exception as e:
            print(f"âš ï¸  Erreur fichier {f}: {e}")

    if not chunks:
        print("âŒ Aucune donnÃ©e chargÃ©e !")
        sys.exit(1)

    print(f"ğŸ”„ Concatenation de {total_rows_seen:,} lignes...")
    df = pd.concat(chunks, ignore_index=True)
    
    # LibÃ©rer la mÃ©moire de la liste chunks
    del chunks 
    import gc
    gc.collect()

    print(f"\n{'='*60}")
    print(f"ğŸ“Š STATISTIQUES (Sur {len(df):,} lignes chargÃ©es)")
    print(f"{'='*60}")
    
    # Optimisation : describe sur float32/int32 est plus rapide
    print(f"\n{df[['count', 'prediction']].describe()}")

    # RÃ©gression linÃ©aire
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ RÃ‰GRESSION LINÃ‰AIRE")
    
    # Scipy gÃ¨re mal les float32 parfois, on convertit juste les colonnes nÃ©cessaires en numpy array pour le calcul
    slope, intercept, r_value, p_value, std_err = stats.linregress(
        df["prediction"].values, df["count"].values
    )

    print(f"Ã‰quation: y = {slope:.4f}x + {intercept:.4f}")
    print(f"RÂ²: {r_value**2:.4f}")

    # RÃ©sidus (Calcul vectorisÃ© rapide)
    residuals = df["count"] - df["prediction"]

    # Top archÃ©types
    print(f"\n{'='*60}")
    print(f"ğŸ† TOP 20 ARCHÃ‰TYPES")
    
    top_archetypes = df.groupby("source")["count_source"].first().sort_values(ascending=False).head(20)
    for i, (arch, count) in enumerate(top_archetypes.items(), 1):
        print(f"  {i:2d}. {arch}: {count:,}")

    # Matrice de corrÃ©lation
    print(f"\n{'='*60}")
    print(f"ğŸ”— CORRÃ‰LATION")
    print(df[["count", "wins", "count_source", "count_target", "prediction"]].corr().to_string())

    # Sauvegardes
    print(f"\n{'='*60}")
    print(f"ğŸ’¾ SAUVEGARDE")
    
    sample_size = min(max_sample_size, len(df))
    df.sample(n=sample_size, random_state=42).to_csv(output_sample, index=False)
    print(f"âœ“ Ã‰chantillon viz ({sample_size}) -> {output_sample}")

    results = {
        "total_edges_analyzed": int(len(df)),
        "regression": {
            "slope": float(slope),
            "intercept": float(intercept),
            "r_squared": float(r_value**2),
        },
        "residuals": {
            "mean": float(residuals.mean()),
            "std": float(residuals.std()),
        },
        "top_archetypes": {k: int(v) for k, v in top_archetypes.items()}
    }

    with open(output_json, "w") as f:
        json.dump(results, f, indent=2)
    print(f"âœ“ Stats JSON -> {output_json}")

if __name__ == "__main__":
    main()